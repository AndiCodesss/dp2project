"""
Crypto Index Construction (Local Verification)
==============================================
A Pandas-based implementation of the "Smart Index" logic for local verification.

This script mimics the exact logic of the Spark pipeline but runs in-memory
using Pandas, which is robust on Windows without Hadoop binaries.

It uses the SAME Parquet data generated by the robust downloader.
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import glob

# =============================================================================
# CONFIGURATION
# =============================================================================

DATA_SOURCE = "crypto_data_parquet" # Partitioned Parquet
CSV_FALLBACK = "crypto_data_4h"

OUTPUT_FOLDER = "output"

CORRELATION_THRESHOLD = 0.85
TOP_N_ASSETS = 20
CANDLES_PER_DAY = 6
ANNUALIZATION_FACTOR = np.sqrt(365 * CANDLES_PER_DAY)

# =============================================================================
# PIPELINE
# =============================================================================

def load_data():
    print("\n--- Stage 1: Loading Data ---")
    
    # Try reading Parquet (efficient)
    if os.path.exists(DATA_SOURCE):
        print(f"Reading Parquet from {DATA_SOURCE}...")
        try:
            # Pandas can read partitioned parquet easily
            df = pd.read_parquet(DATA_SOURCE)
            print(f"Loaded {len(df):,} rows from Parquet")
            return df
        except Exception as e:
            print(f"Parquet loading failed: {e}")
    
    # Fallback to CSV
    print(f"Reading CSVs from {CSV_FALLBACK}...")
    csv_files = glob.glob(os.path.join(CSV_FALLBACK, "*.csv"))
    if not csv_files:
        raise FileNotFoundError("No data found!")
        
    dfs = []
    for f in csv_files:
        try:
            # We need to manually parse CSVs if Parquet failed
            # Basic open/close/timestamp columns
            temp = pd.read_csv(f, usecols=[0, 4], names=["open_time", "close"])
            temp["timestamp"] = pd.to_datetime(temp["open_time"], unit="us")
            
            # Extract symbol from filename
            base = os.path.basename(f)
            symbol = base.split("-")[0]
            temp["symbol"] = symbol
            
            dfs.append(temp[["timestamp", "symbol", "close"]])
        except Exception as e:
            print(f"Error reading {f}: {e}")
            
    df = pd.concat(dfs, ignore_index=True)
    print(f"Loaded {len(df):,} rows from CSVs")
    return df

def calculate_metrics(df):
    print("\n--- Stage 2: Feature Engineering ---")
    
    # Sort
    df = df.sort_values(["symbol", "timestamp"])
    
    # Calculate Log Returns
    df["log_ret"] = df.groupby("symbol")["close"].transform(lambda x: np.log(x / x.shift(1)))
    df = df.dropna(subset=["log_ret"])
    
    # Aggregation
    stats = df.groupby("symbol")["log_ret"].agg(["count", "sum", "std"])
    stats = stats[stats["count"] > 50] # Min data points
    
    # Annualize
    periods_per_year = 365 * CANDLES_PER_DAY
    stats["annual_ret"] = stats["sum"] * (periods_per_year / stats["count"])
    stats["annual_vol"] = stats["std"] * np.sqrt(periods_per_year)
    stats["sharpe"] = stats["annual_ret"] / stats["annual_vol"]
    
    print("Top 10 Metrics:")
    print(stats.sort_values("sharpe", ascending=False).head(10))
    return df, stats

def correlation_pruning(df, metrics_df):
    print("\n--- Stage 3: Correlation Pruning ---")
    
    # Pivot Returns
    # Matrix: Index=Timestamp, Columns=Symbol, Values=LogReturn
    pivot_ret = df.pivot(index="timestamp", columns="symbol", values="log_ret")
    
    # Fill NaN with 0 (assuming no return) or ffill? 
    # 0 is safer for correlation to avoid dropping data, but standard is dropping rows.
    # Let's fill 0.
    pivot_ret = pivot_ret.fillna(0)
    
    # Correlation Matrix
    print(f"Computing correlation for {pivot_ret.shape[1]} assets...")
    corr_matrix = pivot_ret.corr(method="pearson")
    
    # Tournament Filter
    metrics = metrics_df.to_dict("index") # {sym: {sharpe: x, ...}}
    symbols = corr_matrix.columns.tolist()
    dropped = set()
    
    # Iterate upper triangle
    pairs_count = 0
    for i in range(len(symbols)):
        for j in range(i+1, len(symbols)):
            s1 = symbols[i]
            s2 = symbols[j]
            
            val = corr_matrix.iloc[i, j]
            if abs(val) > CORRELATION_THRESHOLD:
                pairs_count += 1
                if s1 in dropped or s2 in dropped:
                    continue
                
                # Compare Sharpe
                sh1 = metrics.get(s1, {}).get("sharpe", -99)
                sh2 = metrics.get(s2, {}).get("sharpe", -99)
                
                if sh1 < sh2:
                    dropped.add(s1)
                else:
                    dropped.add(s2)
                    
    print(f"Evaluated {pairs_count} high-correlation pairs.")
    print(f"Pruned {len(dropped)} redundant assets.")
    
    survivors = [s for s in symbols if s not in dropped]
    return survivors, corr_matrix

def portfolio_construction(survivors, metrics_df):
    print("\n--- Stage 4: Portfolio Construction ---")
    
    # Select survivors
    df = metrics_df.loc[survivors].copy()
    
    # Top N by Sharpe
    df = df.sort_values("sharpe", ascending=False).head(TOP_N_ASSETS)
    
    # Inverse Volatility Weighting
    df["inv_vol"] = 1.0 / df["annual_vol"]
    df["weight"] = df["inv_vol"] / df["inv_vol"].sum()
    df["weight_pct"] = (df["weight"] * 100).round(2)
    
    print("\nFinal Smart Index Composition:")
    print(df[["weight_pct", "sharpe", "annual_vol", "annual_ret"]])
    return df

def main():
    try:
        # 1. Load
        df = load_data()
        
        # 2. Metrics
        df_ret, metrics = calculate_metrics(df)
        
        # 3. Correlation
        survivors, corr_mat = correlation_pruning(df_ret, metrics)
        
        # 4. Portfolio
        final_portfolio = portfolio_construction(survivors, metrics)
        
        # Save Results
        os.makedirs(OUTPUT_FOLDER, exist_ok=True)
        final_portfolio.to_csv(os.path.join(OUTPUT_FOLDER, "smart_index.csv"))
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(corr_mat, cmap="coolwarm", center=0)
        plt.title("Correlation Matrix")
        plt.savefig(os.path.join(OUTPUT_FOLDER, "correlation_matrix.png"))
        print(f"\nâœ… Results saved to {OUTPUT_FOLDER}/")
        
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
