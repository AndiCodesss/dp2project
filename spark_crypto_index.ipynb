{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crypto Index Construction Engine (Scalable Version)\n",
    "\n",
    "A fully distributed **PySpark** analytics engine that constructs a monthly-rebalanced\n",
    "*Smart Index* of cryptocurrencies.\n",
    "\n",
    "## Features\n",
    "- Scales to 2 000+ trading pairs\n",
    "- Fully distributed Spark pipeline — **no Pandas anywhere**\n",
    "- Robust error handling for Windows / local execution\n",
    "\n",
    "## Pipeline Stages\n",
    "| # | Stage | Technique |\n",
    "|---|-------|-----------|\n",
    "| 1 | Ingestion & Temporal Alignment | Spark SQL |\n",
    "| 2 | Feature Engineering / Metrics | Spark Window Functions |\n",
    "| 3 | Correlation Pruning | Spark MLlib (Pearson) |\n",
    "| 3B | Dimensionality Reduction | PCA (Spark MLlib) |\n",
    "| 4 | Asset Clustering | K-Means (Spark MLlib) |\n",
    "| 4C | Return Prediction | Gradient Boosted Trees (Spark MLlib) |\n",
    "| 5 | Portfolio Construction | Inverse-Volatility Weighting |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "try:\n",
    "    findspark.init()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, csv\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")          # non-interactive backend (safe for servers)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, DoubleType, LongType, StringType\n",
    ")\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Data source ──────────────────────────────────────────────────\n",
    "DATA_SOURCE   = \"crypto_data_parquet\"   # Partitioned Parquet\n",
    "CSV_FALLBACK  = \"crypto_data_4h\"        # CSV folder (fallback)\n",
    "OUTPUT_FOLDER = \"output\"\n",
    "\n",
    "# ── Index parameters ────────────────────────────────────────────\n",
    "CORRELATION_THRESHOLD = 0.85\n",
    "TOP_N_ASSETS          = 20\n",
    "CANDLES_PER_DAY       = 6               # 4-hour candles\n",
    "ANNUALIZATION_FACTOR  = np.sqrt(365 * CANDLES_PER_DAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"CryptoIndexEngine\")\n",
    "         .config(\"spark.driver.memory\", \"4g\")\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"20\")\n",
    "         .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 — Data Ingestion & Temporal Alignment\n",
    "\n",
    "Read raw OHLCV candle data (Parquet preferred, CSV fallback) into a long\n",
    "Spark DataFrame with columns `(timestamp, symbol, close)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1A  Read raw data ────────────────────────────────────────────\n",
    "if os.path.exists(DATA_SOURCE) and os.listdir(DATA_SOURCE):\n",
    "    print(f\"Reading Parquet from: {DATA_SOURCE}\")\n",
    "    raw_df = spark.read.parquet(DATA_SOURCE)\n",
    "\n",
    "elif os.path.exists(CSV_FALLBACK):\n",
    "    print(f\"Reading CSV from: {CSV_FALLBACK}\")\n",
    "    csv_files = glob.glob(os.path.join(CSV_FALLBACK, \"*.csv\"))\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSVs in {CSV_FALLBACK}\")\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"datetime\",        StringType(),  True),\n",
    "        StructField(\"open\",            DoubleType(),  True),\n",
    "        StructField(\"high\",            DoubleType(),  True),\n",
    "        StructField(\"low\",             DoubleType(),  True),\n",
    "        StructField(\"close\",           DoubleType(),  True),\n",
    "        StructField(\"volume\",          DoubleType(),  True),\n",
    "        StructField(\"quote_volume\",    DoubleType(),  True),\n",
    "        StructField(\"trades\",          LongType(),    True),\n",
    "        StructField(\"taker_buy_base\",  DoubleType(),  True),\n",
    "        StructField(\"taker_buy_quote\", DoubleType(),  True),\n",
    "        StructField(\"ignore\",          DoubleType(),  True),\n",
    "    ])\n",
    "\n",
    "    raw_df = spark.read.csv(csv_files, schema=schema, header=True)\n",
    "    raw_df = raw_df.withColumn(\"_filename\", F.input_file_name())\n",
    "    raw_df = raw_df.withColumn(\n",
    "        \"symbol\",\n",
    "        F.regexp_extract(F.col(\"_filename\"), r\"([A-Z0-9]+)-\\d+h-\", 1),\n",
    "    )\n",
    "    raw_df = raw_df.withColumn(\n",
    "        \"timestamp\",\n",
    "        F.to_timestamp(F.col(\"datetime\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "    )\n",
    "else:\n",
    "    raise FileNotFoundError(\"No data source found!\")\n",
    "\n",
    "# ── 1B  Select & cache ─────────────────────────────────────────\n",
    "long_df = (\n",
    "    raw_df\n",
    "    .select(F.col(\"timestamp\"), F.col(\"symbol\"), F.col(\"close\").cast(\"double\"))\n",
    "    .filter(F.col(\"close\").isNotNull())\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "row_count = long_df.count()\n",
    "symbols   = sorted([r.symbol for r in long_df.select(\"symbol\").distinct().collect()])\n",
    "print(f\"Loaded {row_count:,} rows across {len(symbols)} assets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1C  Pivot to wide (timestamp × symbol) matrix ───────────────\n",
    "master_matrix = (\n",
    "    long_df\n",
    "    .groupBy(\"timestamp\")\n",
    "    .pivot(\"symbol\", symbols)\n",
    "    .agg(F.first(\"close\"))\n",
    "    .orderBy(\"timestamp\")\n",
    "    .cache()\n",
    ")\n",
    "print(f\"Master Matrix: {master_matrix.count()} rows × {len(symbols)} assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 — Feature Engineering & Metrics\n",
    "\n",
    "For every asset we compute:\n",
    "- **Log returns** (4-hour)\n",
    "- **Annualised return** (scaled to 365 × 6 periods / year)\n",
    "- **Annualised volatility**\n",
    "- **Sharpe ratio** (assuming risk-free rate = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_PERIODS = 365 * CANDLES_PER_DAY          # periods per year\n",
    "\n",
    "win = Window.partitionBy(\"symbol\").orderBy(\"timestamp\")\n",
    "\n",
    "returns_df = (\n",
    "    long_df\n",
    "    .withColumn(\"prev_close\", F.lag(\"close\").over(win))\n",
    "    .withColumn(\"log_ret\", F.log(F.col(\"close\") / F.col(\"prev_close\")))\n",
    "    .filter(F.col(\"log_ret\").isNotNull())\n",
    ")\n",
    "\n",
    "metrics = (\n",
    "    returns_df\n",
    "    .groupBy(\"symbol\")\n",
    "    .agg(\n",
    "        F.count(\"log_ret\").alias(\"count\"),\n",
    "        F.sum(\"log_ret\").alias(\"sum_log_ret\"),\n",
    "        F.stddev(\"log_ret\").alias(\"std_log_ret\"),\n",
    "    )\n",
    "    .filter(F.col(\"count\") > 50)                    # minimum data points\n",
    "    .withColumn(\"annual_ret\",\n",
    "                F.col(\"sum_log_ret\") * (ANN_PERIODS / F.col(\"count\")))\n",
    "    .withColumn(\"annual_vol\",\n",
    "                F.col(\"std_log_ret\") * np.sqrt(ANN_PERIODS))\n",
    "    .withColumn(\"sharpe\",\n",
    "                F.col(\"annual_ret\") / F.col(\"annual_vol\"))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(\"Per-asset metrics computed.  Top 5 by Sharpe:\")\n",
    "metrics.orderBy(F.desc(\"sharpe\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 — Correlation Analysis  *(Spark MLlib — Pearson)*\n",
    "\n",
    "We compute the Pearson correlation matrix on **log returns** (not raw prices)\n",
    "so that two assets that trend together in *percentage moves* are correctly\n",
    "identified as correlated.\n",
    "\n",
    "Missing returns are imputed with **0** (equivalent to forward-filling the\n",
    "price: $\\log(P_t / P_{t-1}) = 0 \\Rightarrow P_t = P_{t-1}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build returns matrix (timestamp × symbol) ───────────────────\n",
    "ret_long = (\n",
    "    long_df\n",
    "    .withColumn(\"prev\", F.lag(\"close\").over(win))\n",
    "    .withColumn(\"ret\", F.log(F.col(\"close\") / F.col(\"prev\")))\n",
    "    .filter(F.col(\"ret\").isNotNull())\n",
    "    .select(\"timestamp\", \"symbol\", \"ret\")\n",
    ")\n",
    "\n",
    "wide_ret = (\n",
    "    ret_long\n",
    "    .groupBy(\"timestamp\")\n",
    "    .pivot(\"symbol\", symbols)\n",
    "    .agg(F.first(\"ret\"))\n",
    "    .fillna(0)                              # forward-fill imputation\n",
    ")\n",
    "\n",
    "valid_cols = [c for c in symbols if c in wide_ret.columns]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=valid_cols, outputCol=\"features\")\n",
    "vec_df    = assembler.transform(wide_ret).select(\"features\")\n",
    "\n",
    "print(\"Computing Pearson correlation matrix …\")\n",
    "corr_mat = Correlation.corr(vec_df, \"features\", \"pearson\").head()[0].toArray()\n",
    "print(f\"Correlation matrix shape: {corr_mat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3B — PCA  *(Spark MLlib — Dimensionality Reduction)*\n",
    "\n",
    "Principal Component Analysis decomposes the returns into independent\n",
    "market factors and tells us how many components explain most of the\n",
    "variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMPONENTS = 10\n",
    "\n",
    "pca = PCA(\n",
    "    k=min(N_COMPONENTS, len(valid_cols)),\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"pca_features\",\n",
    ")\n",
    "pca_model    = pca.fit(vec_df)\n",
    "explained_var = pca_model.explainedVariance.toArray()\n",
    "cumsum_var    = np.cumsum(explained_var)\n",
    "\n",
    "print(\"Explained variance by component:\")\n",
    "for i in range(min(5, len(explained_var))):\n",
    "    print(f\"  PC{i+1}: {explained_var[i]*100:6.2f}%  \"\n",
    "          f\"(cumulative: {cumsum_var[i]*100:6.2f}%)\")\n",
    "print(f\"\\nTotal explained by {len(explained_var)} components: \"\n",
    "      f\"{cumsum_var[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 — K-Means Clustering  *(Spark MLlib)*\n",
    "\n",
    "Cluster cryptocurrencies by their risk / return profiles (`annual_ret`,\n",
    "`annual_vol`, `sharpe`).  Features are **standardised** before clustering.\n",
    "\n",
    "From each cluster the highest-Sharpe asset is selected to form a\n",
    "*diversified* portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 5\n",
    "\n",
    "# ── Assemble & scale ────────────────────────────────────────────\n",
    "km_assembler = VectorAssembler(\n",
    "    inputCols=[\"annual_ret\", \"annual_vol\", \"sharpe\"],\n",
    "    outputCol=\"features_raw\",\n",
    ")\n",
    "km_df = km_assembler.transform(metrics)\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\", outputCol=\"km_features\",\n",
    "    withStd=True, withMean=True,\n",
    ")\n",
    "km_df = scaler.fit(km_df).transform(km_df)\n",
    "\n",
    "# ── Fit K-Means ─────────────────────────────────────────────────\n",
    "kmeans       = KMeans(k=N_CLUSTERS, seed=42, featuresCol=\"km_features\",\n",
    "                      predictionCol=\"cluster\")\n",
    "kmeans_model = kmeans.fit(km_df)\n",
    "clustered    = kmeans_model.transform(km_df).cache()\n",
    "\n",
    "# ── Evaluate ─────────────────────────────────────────────────────\n",
    "evaluator  = ClusteringEvaluator(featuresCol=\"km_features\",\n",
    "                                  predictionCol=\"cluster\",\n",
    "                                  metricName=\"silhouette\")\n",
    "silhouette = evaluator.evaluate(clustered)\n",
    "print(f\"Silhouette score: {silhouette:.3f}  (range −1 … 1, higher = better)\")\n",
    "\n",
    "# ── Cluster statistics ───────────────────────────────────────────\n",
    "print(\"\\nCluster statistics:\")\n",
    "(clustered\n",
    " .groupBy(\"cluster\")\n",
    " .agg(\n",
    "     F.count(\"symbol\").alias(\"n\"),\n",
    "     F.round(F.avg(\"sharpe\"), 4).alias(\"avg_sharpe\"),\n",
    "     F.round(F.avg(\"annual_vol\"), 4).alias(\"avg_vol\"),\n",
    "     F.round(F.avg(\"annual_ret\"), 4).alias(\"avg_ret\"),\n",
    " )\n",
    " .orderBy(\"cluster\")\n",
    " .show())\n",
    "\n",
    "# ── Select cluster leaders ───────────────────────────────────────\n",
    "rank_win = Window.partitionBy(\"cluster\").orderBy(F.desc(\"sharpe\"))\n",
    "cluster_leaders = (\n",
    "    clustered\n",
    "    .withColumn(\"_rank\", F.row_number().over(rank_win))\n",
    "    .filter(F.col(\"_rank\") == 1)\n",
    "    .select(\"symbol\", \"cluster\", \"sharpe\", \"annual_ret\", \"annual_vol\")\n",
    ")\n",
    "print(f\"Selected {cluster_leaders.count()} cluster representatives:\")\n",
    "cluster_leaders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4C — Gradient Boosted Trees  *(Spark MLlib — Regression)*\n",
    "\n",
    "Train a GBT regressor to predict the **next-period log return** from\n",
    "technical features (lagged returns, moving-average ratio, rolling\n",
    "volatility).\n",
    "\n",
    "The model is evaluated on a held-out 20 % test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Feature engineering ───────────────────────────────────────────\n",
    "gbt_win = Window.partitionBy(\"symbol\").orderBy(\"timestamp\")\n",
    "\n",
    "gbt_df = (\n",
    "    long_df\n",
    "    .withColumn(\"prev_close\", F.lag(\"close\").over(gbt_win))\n",
    "    .withColumn(\"log_ret\", F.log(F.col(\"close\") / F.col(\"prev_close\")))\n",
    "    .withColumn(\"ret_lag1\", F.lag(\"log_ret\", 1).over(gbt_win))\n",
    "    .withColumn(\"ret_lag2\", F.lag(\"log_ret\", 2).over(gbt_win))\n",
    "    .withColumn(\"ret_lag3\", F.lag(\"log_ret\", 3).over(gbt_win))\n",
    "    .withColumn(\"ma7\",  F.avg(\"close\").over(gbt_win.rowsBetween(-6, 0)))\n",
    "    .withColumn(\"ma30\", F.avg(\"close\").over(gbt_win.rowsBetween(-29, 0)))\n",
    "    .withColumn(\"ma_ratio\", F.col(\"ma7\") / F.col(\"ma30\"))\n",
    "    .withColumn(\"volatility_7d\",\n",
    "                F.stddev(\"log_ret\").over(gbt_win.rowsBetween(-6, 0)))\n",
    "    .withColumn(\"target\", F.lead(\"log_ret\", 1).over(gbt_win))\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "FEATURE_COLS = [\"ret_lag1\", \"ret_lag2\", \"ret_lag3\", \"ma_ratio\", \"volatility_7d\"]\n",
    "print(f\"Engineered {len(FEATURE_COLS)} features: {FEATURE_COLS}\")\n",
    "\n",
    "gbt_assembler = VectorAssembler(inputCols=FEATURE_COLS, outputCol=\"gbt_features\")\n",
    "gbt_df = gbt_assembler.transform(gbt_df).select(\"symbol\", \"gbt_features\", \"target\")\n",
    "\n",
    "train, test = gbt_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train: {train.count():,} rows  |  Test: {test.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Train GBT ────────────────────────────────────────────────────\n",
    "gbt = GBTRegressor(featuresCol=\"gbt_features\", labelCol=\"target\",\n",
    "                   maxIter=20, maxDepth=5, seed=42)\n",
    "gbt_model = gbt.fit(train)\n",
    "\n",
    "predictions = gbt_model.transform(test)\n",
    "rmse = RegressionEvaluator(\n",
    "    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    ").evaluate(predictions)\n",
    "print(f\"GBT test RMSE: {rmse:.6f}\")\n",
    "\n",
    "# ── Feature importance (pure Python — no Pandas) ────────────────\n",
    "importance = gbt_model.featureImportances.toArray()\n",
    "feat_imp   = sorted(zip(FEATURE_COLS, importance.tolist()),\n",
    "                    key=lambda x: -x[1])\n",
    "\n",
    "print(\"\\nFeature importance:\")\n",
    "for feat, imp in feat_imp:\n",
    "    print(f\"  {feat:20s}  {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 — Portfolio Construction\n",
    "\n",
    "### Correlation-based tournament filter\n",
    "1. For every pair whose |ρ| > threshold, **drop** the asset with the\n",
    "   lower Sharpe ratio.\n",
    "2. From the survivors, keep the top-N by Sharpe.\n",
    "3. Assign **inverse-volatility weights** (lower-vol → higher weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Collect metrics to Python dicts (small data — at most ~2 000 rows) ──\n",
    "_rows     = metrics.select(\"symbol\", \"sharpe\", \"annual_vol\").collect()\n",
    "sharpe_map = {r[\"symbol\"]: float(r[\"sharpe\"])     for r in _rows}\n",
    "vol_map    = {r[\"symbol\"]: float(r[\"annual_vol\"]) for r in _rows}\n",
    "\n",
    "# ── Correlation tournament ──────────────────────────────────────\n",
    "idx_to_sym = {i: s for i, s in enumerate(valid_cols)}\n",
    "keep    = set(valid_cols)\n",
    "dropped = set()\n",
    "\n",
    "n_rows, n_cols = corr_mat.shape\n",
    "for i in range(n_rows):\n",
    "    for j in range(i + 1, n_cols):\n",
    "        if abs(corr_mat[i, j]) > CORRELATION_THRESHOLD:\n",
    "            s1, s2 = idx_to_sym[i], idx_to_sym[j]\n",
    "            if s1 in dropped or s2 in dropped:\n",
    "                continue\n",
    "            sh1 = sharpe_map.get(s1)\n",
    "            sh2 = sharpe_map.get(s2)\n",
    "            if sh1 is None or sh2 is None:\n",
    "                continue\n",
    "            loser = s1 if sh1 < sh2 else s2\n",
    "            dropped.add(loser)\n",
    "            keep.discard(loser)\n",
    "\n",
    "print(f\"Correlation filter dropped {len(dropped)} assets, \"\n",
    "      f\"{len(keep)} remain.\")\n",
    "\n",
    "# ── Build final portfolio in Spark ───────────────────────────────\n",
    "keep_list = list(keep)\n",
    "portfolio = (\n",
    "    metrics\n",
    "    .filter(F.col(\"symbol\").isin(keep_list))\n",
    "    .orderBy(F.desc(\"sharpe\"))\n",
    "    .limit(TOP_N_ASSETS)\n",
    "    .withColumn(\"inv_vol\", F.lit(1.0) / F.col(\"annual_vol\"))\n",
    ")\n",
    "\n",
    "_total_inv_vol = portfolio.agg(F.sum(\"inv_vol\")).collect()[0][0]\n",
    "\n",
    "portfolio = portfolio.withColumn(\n",
    "    \"weight\", F.col(\"inv_vol\") / F.lit(_total_inv_vol)\n",
    ").cache()\n",
    "\n",
    "print(f\"\\nFinal portfolio — top {TOP_N_ASSETS} assets:\")\n",
    "portfolio.select(\"symbol\", \"weight\", \"sharpe\", \"annual_vol\").show(TOP_N_ASSETS, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr_mat, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Matrix (Log Returns)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_FOLDER}/correlation_matrix.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_km_rows = clustered.select(\"annual_vol\", \"sharpe\", \"cluster\").collect()\n",
    "_vols     = [float(r[\"annual_vol\"]) for r in _km_rows]\n",
    "_sharpes  = [float(r[\"sharpe\"])     for r in _km_rows]\n",
    "_clusters = [int(r[\"cluster\"])      for r in _km_rows]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sc = plt.scatter(_vols, _sharpes, c=_clusters, cmap=\"viridis\", s=100, alpha=0.6)\n",
    "plt.xlabel(\"Annualised Volatility\")\n",
    "plt.ylabel(\"Sharpe Ratio\")\n",
    "plt.title(\"K-Means Clustering — Risk vs Return Profile\")\n",
    "plt.colorbar(sc, label=\"Cluster\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_FOLDER}/kmeans_clusters_viz.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].bar(range(1, len(explained_var) + 1), explained_var)\n",
    "axes[0].set_xlabel(\"Principal Component\")\n",
    "axes[0].set_ylabel(\"Explained Variance\")\n",
    "axes[0].set_title(\"PCA — Variance per Component\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(1, len(cumsum_var) + 1), cumsum_var * 100, marker=\"o\")\n",
    "axes[1].axhline(y=90, color=\"r\", linestyle=\"--\", label=\"90 % threshold\")\n",
    "axes[1].set_xlabel(\"Number of Components\")\n",
    "axes[1].set_ylabel(\"Cumulative Explained Variance (%)\")\n",
    "axes[1].set_title(\"PCA — Cumulative Variance\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_FOLDER}/pca_scree_plot.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_feats = [f for f, _ in feat_imp]\n",
    "_imps  = [v for _, v in feat_imp]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(_feats, _imps)\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(f\"GBT Feature Importance  (RMSE = {rmse:.6f})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_FOLDER}/gbt_feature_importance.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _spark_to_csv(sdf, path, columns=None):\n",
    "    # Collect a (small) Spark DataFrame and write a single CSV via Python.\n",
    "    cols = columns or sdf.columns\n",
    "    rows = sdf.select(*cols).collect()\n",
    "    with open(path, \"w\", newline=\"\") as fh:\n",
    "        writer = csv.writer(fh)\n",
    "        writer.writerow(cols)\n",
    "        for r in rows:\n",
    "            writer.writerow([r[c] for c in cols])\n",
    "    print(f\"  ✓ {path}\")\n",
    "\n",
    "print(\"Saving outputs …\")\n",
    "\n",
    "# Portfolio\n",
    "_spark_to_csv(portfolio, f\"{OUTPUT_FOLDER}/smart_index.csv\",\n",
    "              [\"symbol\", \"weight\", \"sharpe\", \"annual_vol\"])\n",
    "\n",
    "# K-Means clusters\n",
    "_spark_to_csv(\n",
    "    clustered.select(\"symbol\", \"cluster\", \"sharpe\", \"annual_ret\", \"annual_vol\"),\n",
    "    f\"{OUTPUT_FOLDER}/kmeans_clusters.csv\",\n",
    ")\n",
    "\n",
    "# K-Means portfolio (cluster leaders)\n",
    "_spark_to_csv(cluster_leaders, f\"{OUTPUT_FOLDER}/kmeans_portfolio.csv\")\n",
    "\n",
    "# PCA components\n",
    "with open(f\"{OUTPUT_FOLDER}/pca_components.csv\", \"w\", newline=\"\") as fh:\n",
    "    writer = csv.writer(fh)\n",
    "    writer.writerow([\"component\", \"explained_variance\", \"cumulative_variance\"])\n",
    "    for i in range(len(explained_var)):\n",
    "        writer.writerow([f\"PC{i+1}\", explained_var[i], cumsum_var[i]])\n",
    "print(f\"  ✓ {OUTPUT_FOLDER}/pca_components.csv\")\n",
    "\n",
    "# GBT feature importance\n",
    "with open(f\"{OUTPUT_FOLDER}/gbt_feature_importance.csv\", \"w\", newline=\"\") as fh:\n",
    "    writer = csv.writer(fh)\n",
    "    writer.writerow([\"feature\", \"importance\"])\n",
    "    for feat, imp in feat_imp:\n",
    "        writer.writerow([feat, imp])\n",
    "print(f\"  ✓ {OUTPUT_FOLDER}/gbt_feature_importance.csv\")\n",
    "\n",
    "print(\"\\nAll outputs saved to\", OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}