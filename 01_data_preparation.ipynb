{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "n1m1",
   "metadata": {},
   "source": [
    "# Crypto Data Pipeline: Download and Preparation\n",
    "\n",
    "This notebook downloads 4-hour OHLCV candle data for hundreds of\n",
    "cryptocurrencies from **Binance Public Data** and converts the raw CSVs\n",
    "into partitioned **Parquet** files for efficient Apache Spark processing.\n",
    "\n",
    "**Steps:**\n",
    "1. Discover available trading pairs via the Binance API\n",
    "2. Download monthly 4h candle archives in parallel\n",
    "3. Process raw CSVs (add headers, convert timestamps)\n",
    "4. Convert to year/month-partitioned Parquet using Spark\n",
    "5. Verify the final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1m2",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "n1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "import shutil\n",
    "import glob as globlib\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Tuple, Dict, Set\n",
    "\n",
    "import findspark\n",
    "try:\n",
    "    findspark.init()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, LongType\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1m3",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We target **4-hour candles** from October 2024 through December 2025.\n",
    "The Oct-Dec 2024 window is warmup data that supports a 90-day lookback\n",
    "for the backtest starting in January 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "n1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback pair list (used only when live Binance discovery fails)\n",
    "FALLBACK_TOP_PAIRS = [\n",
    "    # Top 10 by market cap\n",
    "    \"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\", \"XRPUSDT\", \"SOLUSDT\",\n",
    "    \"ADAUSDT\", \"DOGEUSDT\", \"TRXUSDT\", \"AVAXUSDT\", \"LINKUSDT\",\n",
    "    # 11-20\n",
    "    \"DOTUSDT\", \"MATICUSDT\", \"UNIUSDT\", \"LTCUSDT\", \"ATOMUSDT\",\n",
    "    \"ETCUSDT\", \"XLMUSDT\", \"FILUSDT\", \"NEARUSDT\", \"APTUSDT\",\n",
    "    # 21-30\n",
    "    \"ARBUSDT\", \"OPUSDT\", \"VETUSDT\", \"ALGOUSDT\", \"FTMUSDT\",\n",
    "    \"SANDUSDT\", \"MANAUSDT\", \"AXSUSDT\", \"AAVEUSDT\", \"EGLDUSDT\",\n",
    "    # 31-40\n",
    "    \"EOSUSDT\", \"XTZUSDT\", \"THETAUSDT\", \"ICPUSDT\", \"GRTUSDT\",\n",
    "    \"FLOWUSDT\", \"NEOUSDT\", \"MKRUSDT\", \"SNXUSDT\", \"KAVAUSDT\",\n",
    "    # 41-50\n",
    "    \"RNDRUSDT\", \"INJUSDT\", \"SUIUSDT\", \"SEIUSDT\", \"TIAUSDT\",\n",
    "    \"IMXUSDT\", \"LDOUSDT\", \"RUNEUSDT\", \"CFXUSDT\", \"MINAUSDT\",\n",
    "    # 51-60\n",
    "    \"APEUSDT\", \"GMXUSDT\", \"FETUSDT\", \"AGIXUSDT\", \"OCEANUSDT\",\n",
    "    \"WOOUSDT\", \"CRVUSDT\", \"COMPUSDT\", \"LRCUSDT\", \"ENJUSDT\",\n",
    "    # 61-70\n",
    "    \"CHZUSDT\", \"GALAUSDT\", \"DYDXUSDT\", \"ZECUSDT\", \"DASHUSDT\",\n",
    "    \"WAVESUSDT\", \"BATUSDT\", \"ZILUSDT\", \"IOSTUSDT\", \"ONTUSDT\",\n",
    "    # 71-80\n",
    "    \"HOTUSDT\", \"RVNUSDT\", \"ZENUSDT\", \"SCUSDT\", \"ICXUSDT\",\n",
    "    \"ZRXUSDT\", \"SXPUSDT\", \"KSMUSDT\", \"CELRUSDT\", \"ONEUSDT\",\n",
    "    # 81-90\n",
    "    \"QTUMUSDT\", \"ANKRUSDT\", \"SKLUSDT\", \"COTIUSDT\", \"BAKEUSDT\",\n",
    "    \"IOTAUSDT\", \"CTSIUSDT\", \"BANDUSDT\", \"STMXUSDT\", \"OGNUSDT\",\n",
    "    # 91-100\n",
    "    \"NKNUSDT\", \"DENTUSDT\", \"MTLUSDT\", \"REEFUSDT\", \"DGBUSDT\",\n",
    "    \"1INCHUSDT\", \"SUSHIUSDT\", \"YFIUSDT\", \"AUDIOUSDT\", \"CELOUSDT\",\n",
    "]\n",
    "\n",
    "# Cross-sector seed pairs to enforce diversification\n",
    "DIVERSIFIED_SEED_PAIRS = {\n",
    "    \"large_cap\": [\n",
    "        \"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\", \"XRPUSDT\", \"SOLUSDT\",\n",
    "        \"ADAUSDT\", \"TRXUSDT\", \"DOGEUSDT\", \"AVAXUSDT\", \"LINKUSDT\",\n",
    "    ],\n",
    "    \"meme\": [\n",
    "        \"DOGEUSDT\", \"SHIBUSDT\", \"PEPEUSDT\", \"FLOKIUSDT\", \"BONKUSDT\",\n",
    "        \"WIFUSDT\", \"MEMEUSDT\", \"BOMEUSDT\", \"1000SATSUSDT\", \"TURBOUSDT\",\n",
    "    ],\n",
    "    \"stable\": [\"USDCUSDT\", \"FDUSDUSDT\", \"USDPUSDT\", \"TUSDUSDT\", \"DAIUSDT\"],\n",
    "    \"commodity\": [\"PAXGUSDT\", \"XAUTUSDT\"],\n",
    "    \"infra_exchange\": [\n",
    "        \"BNBUSDT\", \"ATOMUSDT\", \"NEARUSDT\", \"DOTUSDT\", \"INJUSDT\",\n",
    "        \"TIAUSDT\", \"SEIUSDT\", \"SUIUSDT\", \"APTUSDT\",\n",
    "    ],\n",
    "    \"defi_lending_dex\": [\n",
    "        \"UNIUSDT\", \"AAVEUSDT\", \"MKRUSDT\", \"LDOUSDT\", \"CRVUSDT\",\n",
    "        \"COMPUSDT\", \"SNXUSDT\", \"RUNEUSDT\", \"DYDXUSDT\",\n",
    "    ],\n",
    "    \"ai_data\": [\"FETUSDT\", \"AGIXUSDT\", \"OCEANUSDT\", \"TAOUSDT\", \"RNDRUSDT\"],\n",
    "    \"gaming_nft\": [\n",
    "        \"IMXUSDT\", \"AXSUSDT\", \"SANDUSDT\", \"MANAUSDT\", \"GALAUSDT\",\n",
    "        \"APEUSDT\", \"ENJUSDT\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Time range\n",
    "START_YEAR, END_YEAR = 2024, 2025\n",
    "START_MONTH, END_MONTH = 10, 12  # Oct 2024 through Dec 2025\n",
    "INTERVAL = \"4h\"\n",
    "\n",
    "# Binance endpoints\n",
    "BASE_URL = \"https://data.binance.vision/data/spot/monthly/klines\"\n",
    "S3_BUCKET_LIST_URL = \"https://s3-ap-northeast-1.amazonaws.com/data.binance.vision\"\n",
    "EXCHANGE_INFO_URL = \"https://api.binance.com/api/v3/exchangeInfo\"\n",
    "TICKER_24HR_URL = \"https://api.binance.com/api/v3/ticker/24hr\"\n",
    "\n",
    "# Output folders\n",
    "CSV_FOLDER = \"crypto_data_4h\"\n",
    "PARQUET_FOLDER = \"crypto_data_parquet\"\n",
    "\n",
    "# Download concurrency\n",
    "MAX_WORKERS = 10\n",
    "DEFAULT_QUOTE_ASSET = \"USDT\"\n",
    "DEFAULT_TARGET_PAIRS = 300\n",
    "\n",
    "# Binance CSV columns (after our timestamp conversion)\n",
    "COLUMNS = [\n",
    "    \"datetime\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "    \"quote_volume\", \"trades\", \"taker_buy_base\",\n",
    "    \"taker_buy_quote\", \"ignore\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1m4",
   "metadata": {},
   "source": [
    "## 3. Pair Universe Discovery\n",
    "\n",
    "We offer four modes for selecting which trading pairs to download:\n",
    "\n",
    "| Mode | Description |\n",
    "|------|-------------|\n",
    "| `top100` | Static fallback list (stable, reproducible) |\n",
    "| `diversified` | Curated seeds + volume-ranked fill up to target count |\n",
    "| `all-usdt` | Every active Binance spot pair quoted in USDT |\n",
    "| `historical-usdt` | All historically listed pairs (includes delisted) |\n",
    "\n",
    "The **diversified** mode is the default. It seeds the universe from eight\n",
    "sectors (large-cap, meme, stablecoin, commodity, infra, DeFi, AI, gaming)\n",
    "and fills the remaining slots by 24-hour quote volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "n1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_year_months(start_year, start_month, end_year, end_month):\n",
    "    \"\"\"Build a list of (year, month) tuples for the download range.\"\"\"\n",
    "    periods = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        m_start = start_month if year == start_year else 1\n",
    "        m_end = end_month if year == end_year else 12\n",
    "        for month in range(m_start, m_end + 1):\n",
    "            periods.append((str(year), f\"{month:02d}\"))\n",
    "    return periods\n",
    "\n",
    "\n",
    "def is_likely_leveraged_token(symbol, quote=\"USDT\"):\n",
    "    \"\"\"Heuristic filter for leveraged tokens like BTCUPUSDT, ETHDOWNUSDT.\"\"\"\n",
    "    if not symbol.endswith(quote):\n",
    "        return False\n",
    "    base = symbol[: -len(quote)]\n",
    "    tags = {\"UP\", \"DOWN\", \"BULL\", \"BEAR\"}\n",
    "    if base in tags:\n",
    "        return True\n",
    "    for suffix in tags:\n",
    "        if base.endswith(suffix) and len(base[: -len(suffix)]) >= 2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def list_s3_common_prefixes(prefix, delimiter=\"/\", timeout=30):\n",
    "    \"\"\"Paginate through the Binance public-data S3 bucket listing.\"\"\"\n",
    "    prefixes, marker = [], \"\"\n",
    "    while True:\n",
    "        params = {\"prefix\": prefix, \"delimiter\": delimiter}\n",
    "        if marker:\n",
    "            params[\"marker\"] = marker\n",
    "        resp = requests.get(S3_BUCKET_LIST_URL, params=params, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        root = ET.fromstring(resp.text)\n",
    "        ns = root.tag.split(\"}\")[0].strip(\"{\") if \"}\" in root.tag else \"\"\n",
    "        tag = (lambda n: f\"{{{ns}}}{n}\") if ns else (lambda n: n)\n",
    "        for cp in root.findall(tag(\"CommonPrefixes\")):\n",
    "            el = cp.find(tag(\"Prefix\"))\n",
    "            if el is not None and el.text:\n",
    "                prefixes.append(el.text)\n",
    "        nm = root.find(tag(\"NextMarker\"))\n",
    "        marker = nm.text if nm is not None and nm.text else \"\"\n",
    "        if not marker:\n",
    "            break\n",
    "    return prefixes\n",
    "\n",
    "\n",
    "def fetch_historical_spot_pairs(quote=\"USDT\", include_leveraged=False):\n",
    "    \"\"\"Discover all historically listed USDT spot pairs from the S3 archive.\"\"\"\n",
    "    raw = list_s3_common_prefixes(\"data/spot/monthly/klines/\")\n",
    "    symbols = [p.rstrip(\"/\").split(\"/\")[-1] for p in raw]\n",
    "    filtered = [s for s in symbols if s.endswith(quote)]\n",
    "    if not include_leveraged:\n",
    "        filtered = [s for s in filtered if not is_likely_leveraged_token(s, quote)]\n",
    "    return sorted(set(filtered))\n",
    "\n",
    "\n",
    "def fetch_active_spot_pairs(quote=\"USDT\", timeout=20):\n",
    "    \"\"\"Fetch currently active spot pairs from the Binance Exchange Info API.\"\"\"\n",
    "    resp = requests.get(EXCHANGE_INFO_URL, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return sorted(\n",
    "        s[\"symbol\"]\n",
    "        for s in data.get(\"symbols\", [])\n",
    "        if s.get(\"status\") == \"TRADING\"\n",
    "        and s.get(\"isSpotTradingAllowed\")\n",
    "        and s.get(\"quoteAsset\") == quote\n",
    "    )\n",
    "\n",
    "\n",
    "def fetch_volume_ranked_pairs(quote=\"USDT\", timeout=20):\n",
    "    \"\"\"Return spot pairs ordered by 24h quote volume (highest first).\"\"\"\n",
    "    resp = requests.get(TICKER_24HR_URL, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    ranked = []\n",
    "    for row in resp.json():\n",
    "        sym = row.get(\"symbol\", \"\")\n",
    "        if sym.endswith(quote):\n",
    "            try:\n",
    "                ranked.append((sym, float(row.get(\"quoteVolume\", 0))))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [sym for sym, _ in ranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "n1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_unique(pair, selected, seen, active):\n",
    "    if pair in active and pair not in seen:\n",
    "        selected.append(pair)\n",
    "        seen.add(pair)\n",
    "\n",
    "\n",
    "def build_diversified_universe(active_pairs, target, quote=\"USDT\"):\n",
    "    \"\"\"\n",
    "    Build a diversified pair list:\n",
    "    1) Seed from each sector category\n",
    "    2) Fill remaining slots by 24h quote volume\n",
    "    \"\"\"\n",
    "    active_set = set(active_pairs)\n",
    "    selected, seen = [], set()\n",
    "\n",
    "    print(\"\\nSeeding diversified universe:\")\n",
    "    for cat, seeds in DIVERSIFIED_SEED_PAIRS.items():\n",
    "        before = len(selected)\n",
    "        for p in seeds:\n",
    "            _add_unique(p, selected, seen, active_set)\n",
    "        print(f\"  {cat}: +{len(selected) - before}\")\n",
    "\n",
    "    # Fill with volume-ranked pairs\n",
    "    for p in fetch_volume_ranked_pairs(quote):\n",
    "        if len(selected) >= target:\n",
    "            break\n",
    "        _add_unique(p, selected, seen, active_set)\n",
    "\n",
    "    # Last resort: alphabetical fill\n",
    "    for p in active_pairs:\n",
    "        if len(selected) >= target:\n",
    "            break\n",
    "        _add_unique(p, selected, seen, active_set)\n",
    "\n",
    "    return selected[:target]\n",
    "\n",
    "\n",
    "def resolve_pair_universe(\n",
    "    mode=\"diversified\",\n",
    "    target=300,\n",
    "    quote=\"USDT\",\n",
    "    include_leveraged_tokens=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Resolve the symbol universe for download.\n",
    "\n",
    "    Modes: top100 | diversified | all-usdt | historical-usdt\n",
    "\n",
    "    include_leveraged_tokens:\n",
    "      - Only relevant for historical-usdt mode.\n",
    "      - Keep False for index research; leveraged tokens are synthetic products and\n",
    "        can distort volatility/return comparisons.\n",
    "    \"\"\"\n",
    "    mode = mode.lower()\n",
    "\n",
    "    if mode == \"top100\":\n",
    "        pairs = [p for p in FALLBACK_TOP_PAIRS if p.endswith(quote)]\n",
    "        print(f\"Pair mode: top100 (static). {len(pairs)} pairs.\")\n",
    "        return pairs\n",
    "\n",
    "    if mode == \"historical-usdt\":\n",
    "        try:\n",
    "            pairs = fetch_historical_spot_pairs(\n",
    "                quote=quote,\n",
    "                include_leveraged=include_leveraged_tokens,\n",
    "            )\n",
    "            print(\n",
    "                f\"Pair mode: historical-usdt. Using {len(pairs)} historical {quote} pairs \"\n",
    "                f\"(include_leveraged_tokens={include_leveraged_tokens}).\"\n",
    "            )\n",
    "            return pairs\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: historical discovery failed ({e}), falling back.\")\n",
    "\n",
    "    try:\n",
    "        active = fetch_active_spot_pairs(quote)\n",
    "        print(f\"Discovered {len(active)} active Binance spot {quote} pairs.\")\n",
    "\n",
    "        if mode == \"all-usdt\":\n",
    "            return active\n",
    "\n",
    "        if mode == \"diversified\":\n",
    "            cap = min(max(1, target), len(active))\n",
    "            pairs = build_diversified_universe(active, cap, quote)\n",
    "            print(f\"Pair mode: diversified. {len(pairs)} pairs (target={cap}).\")\n",
    "            return pairs\n",
    "\n",
    "        raise ValueError(f\"Unknown pair mode: {mode}\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: live discovery failed ({e}), using fallback.\")\n",
    "        return [p for p in FALLBACK_TOP_PAIRS if p.endswith(quote)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1m5",
   "metadata": {},
   "source": [
    "## 4. Download Functions\n",
    "\n",
    "Each monthly archive is a ZIP file hosted on `data.binance.vision`.\n",
    "We download them in parallel, extract the CSV, and immediately convert\n",
    "the raw Binance timestamp (milliseconds since epoch) into a\n",
    "human-readable `YYYY-MM-DD HH:MM:SS` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "n1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binance_timestamp(ts):\n",
    "    \"\"\"Convert a Binance open_time (int) to a datetime string.\n",
    "    Handles seconds, milliseconds, and microsecond precision.\"\"\"\n",
    "    if ts >= 10**15:\n",
    "        seconds = ts / 1e6\n",
    "    elif ts >= 10**12:\n",
    "        seconds = ts / 1e3\n",
    "    else:\n",
    "        seconds = float(ts)\n",
    "    dt = datetime.utcfromtimestamp(seconds)\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "def add_header_to_csv(csv_file, columns):\n",
    "    \"\"\"Rewrite a raw Binance CSV: convert timestamps and prepend a header row.\"\"\"\n",
    "    try:\n",
    "        with open(csv_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        if lines and \"datetime\" in lines[0]:\n",
    "            return  # already processed\n",
    "\n",
    "        tmp = csv_file + \".tmp\"\n",
    "        converted = 0\n",
    "        with open(tmp, \"w\", newline=\"\") as f:\n",
    "            f.write(\",\".join(columns) + \"\\n\")\n",
    "            for line in lines:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                parts = line.strip().split(\",\")\n",
    "                if len(parts) < 12:\n",
    "                    continue\n",
    "                if any(ch.isalpha() for ch in parts[0]):\n",
    "                    continue\n",
    "                dt_str = convert_binance_timestamp(int(parts[0]))\n",
    "                # Skip close_time (parts[6]); keep the rest\n",
    "                row = f\"{dt_str},{parts[1]},{parts[2]},{parts[3]},{parts[4]},{parts[5]},\"\n",
    "                row += f\"{parts[7]},{parts[8]},{parts[9]},{parts[10]},{parts[11]}\\n\"\n",
    "                f.write(row)\n",
    "                converted += 1\n",
    "\n",
    "        if converted == 0:\n",
    "            os.remove(tmp)\n",
    "            raise ValueError(\"No rows converted\")\n",
    "        os.replace(tmp, csv_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        tmp = csv_file + \".tmp\"\n",
    "        if os.path.exists(tmp):\n",
    "            try:\n",
    "                os.remove(tmp)\n",
    "            except OSError:\n",
    "                pass\n",
    "        print(f\"  Error processing {csv_file}: {e}\")\n",
    "\n",
    "\n",
    "def download_file(pair, year, month, interval, folder):\n",
    "    \"\"\"Download and extract one monthly ZIP from Binance Public Data.\"\"\"\n",
    "    file_name = f\"{pair}-{interval}-{year}-{month}.zip\"\n",
    "    url = f\"{BASE_URL}/{pair}/{interval}/{file_name}\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        if resp.status_code == 200:\n",
    "            with zipfile.ZipFile(io.BytesIO(resp.content)) as z:\n",
    "                z.extractall(folder)\n",
    "            csv_path = os.path.join(folder, file_name.replace(\".zip\", \".csv\"))\n",
    "            if os.path.exists(csv_path):\n",
    "                add_header_to_csv(csv_path, COLUMNS)\n",
    "            return (pair, year, month, True, \"OK\")\n",
    "        return (pair, year, month, False, f\"HTTP {resp.status_code}\")\n",
    "    except Exception as e:\n",
    "        return (pair, year, month, False, str(e)[:60])\n",
    "\n",
    "\n",
    "def download_all_data(pairs, periods, interval, folder, workers=10):\n",
    "    \"\"\"Download all pair x month combinations in parallel.\"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    tasks = [(p, y, m) for p in pairs for y, m in periods]\n",
    "    total = len(tasks)\n",
    "    print(f\"\\nDownloading {total} files ({len(pairs)} pairs x {len(periods)} months) ...\")\n",
    "\n",
    "    ok, fails = 0, []\n",
    "    with ThreadPoolExecutor(max_workers=workers) as pool:\n",
    "        futures = {\n",
    "            pool.submit(download_file, p, y, m, interval, folder): (p, y, m)\n",
    "            for p, y, m in tasks\n",
    "        }\n",
    "        for i, fut in enumerate(as_completed(futures), 1):\n",
    "            pair, year, month, success, msg = fut.result()\n",
    "            if success:\n",
    "                ok += 1\n",
    "            else:\n",
    "                fails.append((pair, year, month, msg))\n",
    "            if i % 50 == 0 or i == total:\n",
    "                print(f\"  Progress: {i}/{total} ({ok} OK)\")\n",
    "\n",
    "    print(f\"\\nDownload complete. Success: {ok}, Failed: {len(fails)}\")\n",
    "    return {\"success\": ok, \"failed\": len(fails)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1m6",
   "metadata": {},
   "source": [
    "## 5. Execute Download Pipeline\n",
    "\n",
    "Resolve the pair universe and download all monthly archives.\n",
    "\n",
    "Default is now `historical-usdt` because the backtest needs a realistic 2024-2025 tradable universe (including delisted symbols), not only currently active listings.\n",
    "Leveraged tokens are excluded by default for cleaner index construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "n1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair mode: historical-usdt. Using 598 historical USDT pairs (include_leveraged_tokens=False).\n",
      "\n",
      "Plan: 598 pairs x 15 months = 8970 files\n",
      "\n",
      "Downloading 8970 files (598 pairs x 15 months) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31617/3480073543.py:10: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt = datetime.utcfromtimestamp(seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 50/8970 (33 OK)\n",
      "  Progress: 100/8970 (73 OK)\n",
      "  Progress: 150/8970 (112 OK)\n",
      "  Progress: 200/8970 (161 OK)\n",
      "  Progress: 250/8970 (209 OK)\n",
      "  Progress: 300/8970 (248 OK)\n",
      "  Progress: 350/8970 (270 OK)\n",
      "  Progress: 400/8970 (305 OK)\n",
      "  Progress: 450/8970 (342 OK)\n",
      "  Progress: 500/8970 (380 OK)\n",
      "  Progress: 550/8970 (410 OK)\n",
      "  Progress: 600/8970 (437 OK)\n",
      "  Progress: 650/8970 (472 OK)\n",
      "  Progress: 700/8970 (522 OK)\n",
      "  Progress: 750/8970 (572 OK)\n",
      "  Progress: 800/8970 (608 OK)\n",
      "  Progress: 850/8970 (651 OK)\n",
      "  Progress: 900/8970 (687 OK)\n",
      "  Progress: 950/8970 (701 OK)\n",
      "  Progress: 1000/8970 (733 OK)\n",
      "  Progress: 1050/8970 (777 OK)\n",
      "  Progress: 1100/8970 (804 OK)\n",
      "  Progress: 1150/8970 (843 OK)\n",
      "  Progress: 1200/8970 (879 OK)\n",
      "  Progress: 1250/8970 (894 OK)\n",
      "  Progress: 1300/8970 (919 OK)\n",
      "  Progress: 1350/8970 (957 OK)\n",
      "  Progress: 1400/8970 (982 OK)\n",
      "  Progress: 1450/8970 (1013 OK)\n",
      "  Progress: 1500/8970 (1041 OK)\n",
      "  Progress: 1550/8970 (1088 OK)\n",
      "  Progress: 1600/8970 (1117 OK)\n",
      "  Progress: 1650/8970 (1142 OK)\n",
      "  Progress: 1700/8970 (1157 OK)\n",
      "  Progress: 1750/8970 (1178 OK)\n",
      "  Progress: 1800/8970 (1193 OK)\n",
      "  Progress: 1850/8970 (1243 OK)\n",
      "  Progress: 1900/8970 (1289 OK)\n",
      "  Progress: 1950/8970 (1339 OK)\n",
      "  Progress: 2000/8970 (1374 OK)\n",
      "  Progress: 2050/8970 (1402 OK)\n",
      "  Progress: 2100/8970 (1451 OK)\n",
      "  Progress: 2150/8970 (1493 OK)\n",
      "  Progress: 2200/8970 (1526 OK)\n",
      "  Progress: 2250/8970 (1561 OK)\n",
      "  Progress: 2300/8970 (1585 OK)\n",
      "  Progress: 2350/8970 (1635 OK)\n",
      "  Progress: 2400/8970 (1685 OK)\n",
      "  Progress: 2450/8970 (1705 OK)\n",
      "  Progress: 2500/8970 (1746 OK)\n",
      "  Progress: 2550/8970 (1777 OK)\n",
      "  Progress: 2600/8970 (1821 OK)\n",
      "  Progress: 2650/8970 (1866 OK)\n",
      "  Progress: 2700/8970 (1907 OK)\n",
      "  Progress: 2750/8970 (1935 OK)\n",
      "  Progress: 2800/8970 (1945 OK)\n",
      "  Progress: 2850/8970 (1971 OK)\n",
      "  Progress: 2900/8970 (2009 OK)\n",
      "  Progress: 2950/8970 (2059 OK)\n",
      "  Progress: 3000/8970 (2098 OK)\n",
      "  Progress: 3050/8970 (2140 OK)\n",
      "  Progress: 3100/8970 (2189 OK)\n",
      "  Progress: 3150/8970 (2219 OK)\n",
      "  Progress: 3200/8970 (2234 OK)\n",
      "  Progress: 3250/8970 (2262 OK)\n",
      "  Progress: 3300/8970 (2299 OK)\n",
      "  Progress: 3350/8970 (2319 OK)\n",
      "  Progress: 3400/8970 (2347 OK)\n",
      "  Progress: 3450/8970 (2397 OK)\n",
      "  Progress: 3500/8970 (2442 OK)\n",
      "  Progress: 3550/8970 (2472 OK)\n",
      "  Progress: 3600/8970 (2500 OK)\n",
      "  Progress: 3650/8970 (2524 OK)\n",
      "  Progress: 3700/8970 (2561 OK)\n",
      "  Progress: 3750/8970 (2608 OK)\n",
      "  Progress: 3800/8970 (2625 OK)\n",
      "  Progress: 3850/8970 (2662 OK)\n",
      "  Progress: 3900/8970 (2712 OK)\n",
      "  Progress: 3950/8970 (2757 OK)\n",
      "  Progress: 4000/8970 (2806 OK)\n",
      "  Progress: 4050/8970 (2856 OK)\n",
      "  Progress: 4100/8970 (2894 OK)\n",
      "  Progress: 4150/8970 (2944 OK)\n",
      "  Progress: 4200/8970 (2990 OK)\n",
      "  Progress: 4250/8970 (3014 OK)\n",
      "  Progress: 4300/8970 (3020 OK)\n",
      "  Progress: 4350/8970 (3053 OK)\n",
      "  Progress: 4400/8970 (3078 OK)\n",
      "  Progress: 4450/8970 (3117 OK)\n",
      "  Progress: 4500/8970 (3137 OK)\n",
      "  Progress: 4550/8970 (3177 OK)\n",
      "  Progress: 4600/8970 (3207 OK)\n",
      "  Progress: 4650/8970 (3257 OK)\n",
      "  Progress: 4700/8970 (3302 OK)\n",
      "  Progress: 4750/8970 (3352 OK)\n",
      "  Progress: 4800/8970 (3387 OK)\n",
      "  Progress: 4850/8970 (3417 OK)\n",
      "  Progress: 4900/8970 (3442 OK)\n",
      "  Progress: 4950/8970 (3477 OK)\n",
      "  Progress: 5000/8970 (3496 OK)\n",
      "  Progress: 5050/8970 (3510 OK)\n",
      "  Progress: 5100/8970 (3530 OK)\n",
      "  Progress: 5150/8970 (3566 OK)\n",
      "  Progress: 5200/8970 (3586 OK)\n",
      "  Progress: 5250/8970 (3599 OK)\n",
      "  Progress: 5300/8970 (3631 OK)\n",
      "  Progress: 5350/8970 (3673 OK)\n",
      "  Progress: 5400/8970 (3711 OK)\n",
      "  Progress: 5450/8970 (3746 OK)\n",
      "  Progress: 5500/8970 (3759 OK)\n",
      "  Progress: 5550/8970 (3788 OK)\n",
      "  Progress: 5600/8970 (3816 OK)\n",
      "  Progress: 5650/8970 (3865 OK)\n",
      "  Progress: 5700/8970 (3891 OK)\n",
      "  Progress: 5750/8970 (3925 OK)\n",
      "  Progress: 5800/8970 (3970 OK)\n",
      "  Progress: 5850/8970 (3997 OK)\n",
      "  Progress: 5900/8970 (4041 OK)\n",
      "  Progress: 5950/8970 (4080 OK)\n",
      "  Progress: 6000/8970 (4115 OK)\n",
      "  Progress: 6050/8970 (4133 OK)\n",
      "  Progress: 6100/8970 (4159 OK)\n",
      "  Progress: 6150/8970 (4209 OK)\n",
      "  Progress: 6200/8970 (4246 OK)\n",
      "  Progress: 6250/8970 (4280 OK)\n",
      "  Progress: 6300/8970 (4330 OK)\n",
      "  Progress: 6350/8970 (4380 OK)\n",
      "  Progress: 6400/8970 (4415 OK)\n",
      "  Progress: 6450/8970 (4459 OK)\n",
      "  Progress: 6500/8970 (4492 OK)\n",
      "  Progress: 6550/8970 (4512 OK)\n",
      "  Progress: 6600/8970 (4546 OK)\n",
      "  Progress: 6650/8970 (4566 OK)\n",
      "  Progress: 6700/8970 (4616 OK)\n",
      "  Progress: 6750/8970 (4665 OK)\n",
      "  Progress: 6800/8970 (4703 OK)\n",
      "  Progress: 6850/8970 (4745 OK)\n",
      "  Progress: 6900/8970 (4779 OK)\n",
      "  Progress: 6950/8970 (4820 OK)\n",
      "  Progress: 7000/8970 (4856 OK)\n",
      "  Progress: 7050/8970 (4896 OK)\n",
      "  Progress: 7100/8970 (4927 OK)\n",
      "  Progress: 7150/8970 (4954 OK)\n",
      "  Progress: 7200/8970 (4995 OK)\n",
      "  Progress: 7250/8970 (5022 OK)\n",
      "  Progress: 7300/8970 (5050 OK)\n",
      "  Progress: 7350/8970 (5100 OK)\n",
      "  Progress: 7400/8970 (5132 OK)\n",
      "  Progress: 7450/8970 (5175 OK)\n",
      "  Progress: 7500/8970 (5211 OK)\n",
      "  Progress: 7550/8970 (5259 OK)\n",
      "  Progress: 7600/8970 (5307 OK)\n",
      "  Progress: 7650/8970 (5329 OK)\n",
      "  Progress: 7700/8970 (5355 OK)\n",
      "  Progress: 7750/8970 (5384 OK)\n",
      "  Progress: 7800/8970 (5428 OK)\n",
      "  Progress: 7850/8970 (5463 OK)\n",
      "  Progress: 7900/8970 (5495 OK)\n",
      "  Progress: 7950/8970 (5525 OK)\n",
      "  Progress: 8000/8970 (5559 OK)\n",
      "  Progress: 8050/8970 (5579 OK)\n",
      "  Progress: 8100/8970 (5613 OK)\n",
      "  Progress: 8150/8970 (5644 OK)\n",
      "  Progress: 8200/8970 (5669 OK)\n",
      "  Progress: 8250/8970 (5699 OK)\n",
      "  Progress: 8300/8970 (5733 OK)\n",
      "  Progress: 8350/8970 (5761 OK)\n",
      "  Progress: 8400/8970 (5804 OK)\n",
      "  Progress: 8450/8970 (5843 OK)\n",
      "  Progress: 8500/8970 (5872 OK)\n",
      "  Progress: 8550/8970 (5891 OK)\n",
      "  Progress: 8600/8970 (5935 OK)\n",
      "  Progress: 8650/8970 (5960 OK)\n",
      "  Progress: 8700/8970 (5998 OK)\n",
      "  Progress: 8750/8970 (6039 OK)\n",
      "  Progress: 8800/8970 (6052 OK)\n",
      "  Progress: 8850/8970 (6090 OK)\n",
      "  Progress: 8900/8970 (6124 OK)\n",
      "  Progress: 8950/8970 (6164 OK)\n",
      "  Progress: 8970/8970 (6169 OK)\n",
      "\n",
      "Download complete. Success: 6169, Failed: 2801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'success': 6169, 'failed': 2801}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Historical universe is the default for the study:\n",
    "# - includes pairs that existed in 2024/2025 (not only currently active ones)\n",
    "# - excludes leveraged tokens to keep assets economically comparable\n",
    "PAIR_MODE = \"historical-usdt\"\n",
    "INCLUDE_LEVERAGED_TOKENS = False\n",
    "\n",
    "pairs = resolve_pair_universe(\n",
    "    mode=PAIR_MODE,\n",
    "    target=DEFAULT_TARGET_PAIRS,\n",
    "    include_leveraged_tokens=INCLUDE_LEVERAGED_TOKENS,\n",
    ")\n",
    "periods = generate_year_months(START_YEAR, START_MONTH, END_YEAR, END_MONTH)\n",
    "print(f\"\\nPlan: {len(pairs)} pairs x {len(periods)} months = {len(pairs) * len(periods)} files\")\n",
    "\n",
    "download_all_data(pairs, periods, INTERVAL, CSV_FOLDER, MAX_WORKERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c5510",
   "metadata": {},
   "source": [
    "### Student Interpretation\n",
    "\n",
    "The downloader found a large historical universe (**598 USDT pairs**) and scheduled **8,970** pair-month files.\n",
    "This is expected for a historical backtest because we include delisted symbols, so some monthly files naturally do not exist.\n",
    "\n",
    "In my run, the progress output confirms the job completed end-to-end and produced a broad raw dataset for 2024-10 to 2025-12.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1m7",
   "metadata": {},
   "source": [
    "## 6. Convert CSV to Parquet with Spark\n",
    "\n",
    "We use Spark to read all processed CSVs, extract the trading-pair symbol\n",
    "from each filename, and write the data as **Snappy-compressed, year/month\n",
    "partitioned Parquet**. This format enables efficient predicate push-down\n",
    "when loading specific time ranges later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "n1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.1.1\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"CryptoDataPrep\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "n1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet written to: crypto_data_parquet/\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"datetime\",        StringType(),  True),\n",
    "    StructField(\"open\",            DoubleType(),  True),\n",
    "    StructField(\"high\",            DoubleType(),  True),\n",
    "    StructField(\"low\",             DoubleType(),  True),\n",
    "    StructField(\"close\",           DoubleType(),  True),\n",
    "    StructField(\"volume\",          DoubleType(),  True),\n",
    "    StructField(\"quote_volume\",    DoubleType(),  True),\n",
    "    StructField(\"trades\",          LongType(),    True),\n",
    "    StructField(\"taker_buy_base\",  DoubleType(),  True),\n",
    "    StructField(\"taker_buy_quote\", DoubleType(),  True),\n",
    "    StructField(\"ignore\",          DoubleType(),  True),\n",
    "])\n",
    "\n",
    "csv_path = os.path.join(CSV_FOLDER, \"*.csv\")\n",
    "df = spark.read.csv(csv_path, schema=schema, header=True)\n",
    "\n",
    "# Extract the trading pair symbol from the filename\n",
    "# e.g.  .../BTCUSDT-4h-2025-01.csv  ->  BTCUSDT\n",
    "df = df.withColumn(\"_fname\", F.input_file_name())\n",
    "df = df.withColumn(\n",
    "    \"symbol\",\n",
    "    F.regexp_extract(\"_fname\", r\"([A-Z0-9]+USDT)-\\d+h-\", 1),\n",
    ")\n",
    "\n",
    "# Parse the datetime string into a proper Spark TimestampType\n",
    "df = df.withColumn(\"timestamp\", F.to_timestamp(\"datetime\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Partition columns\n",
    "df = df.withColumn(\"year\", F.year(\"timestamp\"))\n",
    "df = df.withColumn(\"month\", F.month(\"timestamp\"))\n",
    "\n",
    "# Keep only the columns we need\n",
    "output = df.select(\n",
    "    \"timestamp\", \"symbol\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "    \"year\", \"month\",\n",
    ").filter(F.col(\"close\").isNotNull())\n",
    "\n",
    "# Write partitioned Parquet (overwrites any previous output)\n",
    "if os.path.exists(PARQUET_FOLDER):\n",
    "    shutil.rmtree(PARQUET_FOLDER)\n",
    "\n",
    "output.write.partitionBy(\"year\", \"month\").parquet(PARQUET_FOLDER)\n",
    "print(f\"Parquet written to: {PARQUET_FOLDER}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002fbc7",
   "metadata": {},
   "source": [
    "### Student Interpretation\n",
    "\n",
    "At this stage we convert CSV files into partitioned Parquet (`year`, `month`) for efficient Spark reads.\n",
    "This is an important big-data design choice: we reduce repeated CSV parsing cost and enable partition pruning during model and backtest windows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1m8",
   "metadata": {},
   "source": [
    "## 7. Verify Parquet Dataset\n",
    "\n",
    "Read the freshly written Parquet back into Spark and check row counts,\n",
    "symbol counts, and the date range covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "n1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:  1,110,662\n",
      "Symbols:     501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:====================================================>   (61 + 4) / 65]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|year|month|count|\n",
      "+----+-----+-----+\n",
      "|2024|   10|71220|\n",
      "|2024|   11|69295|\n",
      "|2024|   12|72108|\n",
      "|2025|    1|72718|\n",
      "|2025|    2|66597|\n",
      "|2025|    3|74209|\n",
      "|2025|    4|71889|\n",
      "|2025|    5|73845|\n",
      "|2025|    6|72556|\n",
      "|2025|    7|74985|\n",
      "|2025|    8|75880|\n",
      "|2025|    9|75109|\n",
      "|2025|   10|80037|\n",
      "|2025|   11|78781|\n",
      "|2025|   12|81433|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquet_df = spark.read.parquet(PARQUET_FOLDER)\n",
    "total_rows = parquet_df.count()\n",
    "n_symbols = parquet_df.select(\"symbol\").distinct().count()\n",
    "\n",
    "print(f\"Total rows:  {total_rows:,}\")\n",
    "print(f\"Symbols:     {n_symbols}\")\n",
    "\n",
    "# Show month distribution\n",
    "parquet_df.groupBy(\"year\", \"month\").count().orderBy(\"year\", \"month\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b02b5",
   "metadata": {},
   "source": [
    "### Student Interpretation\n",
    "\n",
    "The verification output confirms the prepared dataset size is around **1.11 million rows** across **~500 symbols**.\n",
    "This is large enough to demonstrate scalable Spark processing while still fitting on our local WSL setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "n1c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped. Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped. Data preparation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
